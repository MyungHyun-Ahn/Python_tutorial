결정트리

예/아니오 > 그 질문을 답해가며 분류하는 알고리즘

같은 속성을 여러번 사용 가능

하나의 시작 질문에서 뻗어나감
내려갈 때마다 예/아니오 중 선택

박스 > Node 노드
맨 위 노드 > root 노드 /뿌리
제일 끝 노드 > leaf 노드 /잎

학습 데이터를 보면서 각 노드를 정하기

지니 불순도 gini impurity

데이터 셋 안에 서로 다른 분류들이 얼만큼 섞여있는지를 나타냄

GI = 1-p(flu)^2 - p(not_flu)^2
flu > 독감일 확률 not_flu > 독감이 아닐 확률

지니 불순도가 낮을수록 순수한 데이터
지니 불순도가 높을수록 불순한 데이터

데이터 셋 안에 서로 다른 분류들이 얼만큼 섞여있는지를 나타냄
작을수록 데이터 셋이 순수하고 클수록 데이터 셋이 불순함

결정 트리 만들기

속성들 >> 예측 >> 목표변수

root node 만들기

좋은 분류 노드는 최대한 많은 학습 데이터 예측을 맞춘다.
>
데이터 셋이 독감 데이터가 많을수록 좋다.
>
데이터 셋이 순수할수록(지니 불순도가 낮을수록) 좋다.

불순도가 높다는건 데이터가 많이 섞여있다는 뜻
만약 분류 노드를 만든다면 가장 많은 분류로 만든다.

좋은 질문은 데이터를 잘 나눠서 아래 노드들이 분류하기 쉽게 만들어준다.
>
질문으로 나뉜 데이터 셋이 순수할수록(지니 불순도가 낮을수록) 더 좋다.

가장 불순도가 낮을 것을 루트 노드로 고른다.

결정트리

장점 : 데이터를 분류하는 방법이 직관적이다, 쉽게 해석이 가능하다

속성 중요도 Feature importance

노드 중요도 node importance 
    > n : 노드까지 오는 데이터 수 GI : 노드 지니 불순도
NI = n/m * GI - n(left)/m * GI(left) - n(right)/m * GI(right)
      > m : 전체 학습 데이터 수        왼쪽 오른쪽도 똑같이 계산

    위 노드에서 아래 노드로 내려오면서, 불순도가 얼마나 줄어들었는지를 계산

특정 변수가 불순도를 얼마나 낮췄는지 > 그 변수가 얼마나 중요한지!

결정 트리 단점

The Elements of Statistcal Learning

결정 트리는 이상적이 머신 러닝 모델이 되기 힘든 한 가지 특징을 갖는다. 바로 부정확성이다.
> 응용하면 성능이 좋은 다른 모델들을 만들 수 있음.

앙상블 ensemble 합주단

여러 독립적인 객체들이 만들어내는 조화로운 단체

하나의 모델을 쓰는 대신 수많은 모델들을 사용해 종합적인 판단을 하는 것

ex) 다수결 판단

수많은 모델들을 만들고 이 모델들의 예측을 합쳐서 종합적인 예측을 하는 기법

랜덤 포레스트

트리 모델들을 임의로 많이 만들어서 다수결 투표로 결과를 종합하는 알고리즘

Bootstrapping : 갖고 있는 데이터 셋으로 다른 데이터 셋을 만들어내는 방법

모든 모델을 정확히 똑같은 데이터 셋으로 학습시키면 결과 다양성 떨어질 수 있음
이 문제를 예방하기 위해, 각 모델을 임의로 만들어준 Bootstrap 데이터 셋으로 학습시킨다.

Bootstrap 데이터 셋을 만들어 내고, 모델들의 결정을 합친다 (aggregating)
Bootstrap aggregating 줄여서는 Bagging 이라고도 부른다.

랜덤 포레스트는 Bagging 을 다루는 예시 중 하나

결정 트리 임의로 만들기

Bootstrapping을 사용해서 임의로 데이터 셋을 만든다.
결정 트리를 만들 때 속성을 임의로 고르면서 만든다.
>> 여러번 반복

만들어 놓은 트리의 예측을 다수결 투표로 결정

에다부스트

Boosting

Bagging Bootstrap aggregating

임의로 Bootstrap 데이터 셋들을 만든다.
Bootstrap 데이터 셋을 사용해서 수많은 모델들을 만든다.
이 모델들의 예측을 종합(aggregating)한다.

Boosting "~전보다 더 크거나 높게 하다."

일부러 성능이 안 좋은 모델들을 사용한다.
더 먼저 만든 모델들의 성능이, 뒤에 있는 모델이 사용할 데이터 셋을 바꾼다.
모델들의 예측을 종합할 때, 성능이 좋은 모델의 예측을 더 반영한다.

핵심 : 성능이 안좋은 약한 학습자 (weak learner)들을 합쳐서 성능을 극대화한다!

에다 부스트에서는 깊이가 얕은 트리를 사용한다.

성능이 좋지 않은 결정 스텀프를 많이 만든다. (weak learners)
각 스텀프는 전에 왔던 스텀프들이 틀린 데이터들을 더 중요하게 맞춘다.
예측을 종합할 때, 성능이 좋은 스텀프의 의견 비중을 더 높게 반영 한다.

스텀프 성능 계산하기

중요도의 합은 항상 1
각 분류/질문들의 불순도 비교해서 스텀프를 만든다.
                          total_error : 틀리게 예측한 데이터 중요도의 합
스텀프 성능 : 1/2 * log( (1-total_error)/total_error )

total_error = 1 > 모든 데이터를 틀리게 예측했을 때
total_error = 0 > 모든 데이터를 맞게 예측했을 때
total_error = 0.5 > 데이터를 딱 절반만 맞췄을 때 > 성능이 의미가 없으므로 0

잘 맞출수록, 또는 못 맞출수록 성능을 기하급수적으로 늘리고 줄여준다.

스텀프의 생성과 성능

첫 번째 스텀프는 결정 트리를 만들 때처럼 지니 불순도를 써서 만든다.
모든 데이터는 중요도가 있다. 중요도는 total_error를 계산하는데 사용한다.
total_error를 이용하면 스텀프의 성능을 계산할 수 있다.

데이터 중요도 업데이트 하기

틀린 데이터 > 중요도를 늘려준다.
맞은 데이터 > 중요도를 줄여준다.

틀리게 예측한 데이터 중요도 : weight(new) = weight(old) * e^P(tree)

맞게 예측한 데이터 중요도 : weight(new) = weight(old) * e^-P(tree)

>> 데이터를 업데이트 해도 중요도의 합은 항상 1
>> 중요도의 합이 1이 되게 하기 위해서
>> 각 중요도마다 : 중요도 / 모든 중요도 합 을 해준다.

스텀프 추가하기

중요도를 이용해서 만듬.

첫 데이터부터 중요도를 이용해서 범위를 나누어줌.
> 0과 1 사이 임의 숫자를 고른다.
> 그 범위를 갖는 데이터를 고른다.

>> 중요도가 높은 데이터는 범위가 크기 때문에 고를 확률이 높음.

전 모델의 틀린 데이터의 중요도는 올렸고, 맞은 데이터의 중요도는 낮췄다.
중요도를 이용해서 새로운 스텀프가 사용할 데이터 셋을 만들었다.
새로운 데이터 셋으로 스텀프를 학습시켜서 추가했다.

>> 뒤 스텀프는 앞 스텀프의 실수를 좀 더 잘 맞추게 된다.

에다 부스트 예측

에다 부스트는 성능이 좋은 스텀프의 의견을 더 들어줌.

