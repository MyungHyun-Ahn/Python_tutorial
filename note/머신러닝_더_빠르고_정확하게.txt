데이터 전처리

데이터를 더 좋은 형식으로 만들어 주는 것

Feature Scaling : 입력변수 크기를 조정해준다

ex : 변수의 단위 차이가 너무 클 때 조정

경사 하강법을 좀 더 빨리할 수 있게 도와준다.

min-max normalization

최솟값, 최댓값을 이용해서 데이터의 크기를 0과 1사이로 바꾸어준다.

y = ( x - min ) / ( max - min )

데이터가 커지면 손실이 급격히 변한다.

feature scaling을 하게되면 경사하강그래프가 원에 가까워진다. & 최소점 방향으로 일직선에 가깝게 내려온다.

선형 회귀 뿐만 아니라 경사 하강법을 사용하는 모든 알고리즘의 속도를 빠르게 해준다.

표준화 standardization

One-hot Encoding

머신러닝에 사용하는 데이터 : 수치형 데이터 / 범주형 데이터

범주형 데이터를 수치로 바꾸어주어야함 >> 숫자로 표현

그렇게 하기 위해 One-hot Encoding 사용

각 카테고리를 하나의 새로운 열로 만들어주는 방법

열의 값을 0 또는 1로 채워줌으로 범주형 데이터에 크고 작은 관계가 생기는 것을 막을 수 있음

정규화 Regularization

편향 / 분산

직선 모델 보다는 곡선 모델이 더 정확할 때도 있음
>> 편향 Bias 가 높다고 한다.
>> 편향이 낮은 모델 구불구불

데이터 셋 별로 모델의 성능 차이가 많이 나면 분산이 높다.

곡선 모델 >> 성능 차이가 크다
직선 모델 >> 성능 차이가 작다 >> 분산이 작다.

편향이 높은 모델은 너무 간단해서 주어진 데이터의 관계를 잘 학습하지 못한다.
편향이 낮은 모델은 주어진 데이터의 관계를 아주 잘 학습한다.
분산은 다양한 테스트 데이터가 주어졌을 때 모델의 성능이 얼마나 일관적인지를 나타낸다.

일반적으로 편향과 분산은 하나가 줄어들수록 하나는 늘어나는 관계
> 편향 분산 트레이드오프 Bias Variance Tradeoff
> 편향과 분산, 과소적합과 과적합의 적당한 밸런스를 찾아내야 함

직선 모델 

복잡도가 떨어지기 때문에 곡선 관계를 학습할 수 없다.
어떤 데이터가 주어져도 일관적인 성능을 낸다.
> 편향이 높고, 분산이 낮은 모델 > 트레이닝 데이터에 과소적합 되었다. Underfit

곡선 모델

training 데이터에 대한 성능은 아주 높다.
처음보는 testing 데이터에 대한 성능은 떨어진다.
> 편향이 낮고, 분산이 높은 모델 > 트레이닝 데이터에 과적합 되었다. Overfit





