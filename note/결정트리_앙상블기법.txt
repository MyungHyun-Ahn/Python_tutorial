결정트리

예/아니오 > 그 질문을 답해가며 분류하는 알고리즘

같은 속성을 여러번 사용 가능

하나의 시작 질문에서 뻗어나감
내려갈 때마다 예/아니오 중 선택

박스 > Node 노드
맨 위 노드 > root 노드 /뿌리
제일 끝 노드 > leaf 노드 /잎

학습 데이터를 보면서 각 노드를 정하기

지니 불순도 gini impurity

데이터 셋 안에 서로 다른 분류들이 얼만큼 섞여있는지를 나타냄

GI = 1-p(flu)^2 - p(not_flu)^2
flu > 독감일 확률 not_flu > 독감이 아닐 확률

지니 불순도가 낮을수록 순수한 데이터
지니 불순도가 높을수록 불순한 데이터

데이터 셋 안에 서로 다른 분류들이 얼만큼 섞여있는지를 나타냄
작을수록 데이터 셋이 순수하고 클수록 데이터 셋이 불순함

결정 트리 만들기

속성들 >> 예측 >> 목표변수

root node 만들기

좋은 분류 노드는 최대한 많은 학습 데이터 예측을 맞춘다.
>
데이터 셋이 독감 데이터가 많을수록 좋다.
>
데이터 셋이 순수할수록(지니 불순도가 낮을수록) 좋다.

불순도가 높다는건 데이터가 많이 섞여있다는 뜻
만약 분류 노드를 만든다면 가장 많은 분류로 만든다.

좋은 질문은 데이터를 잘 나눠서 아래 노드들이 분류하기 쉽게 만들어준다.
>
질문으로 나뉜 데이터 셋이 순수할수록(지니 불순도가 낮을수록) 더 좋다.

가장 불순도가 낮을 것을 루트 노드로 고른다.

결정트리

장점 : 데이터를 분류하는 방법이 직관적이다, 쉽게 해석이 가능하다

속성 중요도 Feature importance

노드 중요도 node importance 
    > n : 노드까지 오는 데이터 수 GI : 노드 지니 불순도
NI = n/m * GI - n(left)/m * GI(left) - n(right)/m * GI(right)
      > m : 전체 학습 데이터 수        왼쪽 오른쪽도 똑같이 계산

    위 노드에서 아래 노드로 내려오면서, 불순도가 얼마나 줄어들었는지를 계산

특정 변수가 불순도를 얼마나 낮췄는지 > 그 변수가 얼마나 중요한지!

결정 트리 단점

The Elements of Statistcal Learning

결정 트리는 이상적이 머신 러닝 모델이 되기 힘든 한 가지 특징을 갖는다. 바로 부정확성이다.
> 응용하면 성능이 좋은 다른 모델들을 만들 수 있음.

앙상블 ensemble 합주단

여러 독립적인 객체들이 만들어내는 조화로운 단체

하나의 모델을 쓰는 대신 수많은 모델들을 사용해 종합적인 판단을 하는 것

ex) 다수결 판단

수많은 모델들을 만들고 이 모델들의 예측을 합쳐서 종합적인 예측을 하는 기법

랜덤 포레스트

트리 모델들을 임의로 많이 만들어서 다수결 투표로 결과를 종합하는 알고리즘

Bootstrapping : 갖고 있는 데이터 셋으로 다른 데이터 셋을 만들어내는 방법

모든 모델을 정확히 똑같은 데이터 셋으로 학습시키면 결과 다양성 떨어질 수 있음
이 문제를 예방하기 위해, 각 모델을 임의로 만들어준 Bootstrap 데이터 셋으로 학습시킨다.

Bootstrap 데이터 셋을 만들어 내고, 모델들의 결정을 합친다 (aggregating)
Bootstrap aggregating 줄여서는 Bagging 이라고도 부른다.

랜덤 포레스트는 Bagging 을 다루는 예시 중 하나

결정 트리 임의로 만들기

Bootstrapping을 사용해서 임의로 데이터 셋을 만든다.
결정 트리를 만들 때 속성을 임의로 고르면서 만든다.
>> 여러번 반복

만들어 놓은 트리의 예측을 다수결 투표로 결정